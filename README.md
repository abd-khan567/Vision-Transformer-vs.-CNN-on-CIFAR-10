# Vision-Transformer-vs.-CNN-on-CIFAR-10
1. Custom GAN & VAE on CIFAR-10 (cats/dogs): Showed the gener-
ator, Siamese discriminator, and VAE encoder-decoder in text form.

2. CycleGAN for Face Sketches: Summarized generator (ResNet blocks)
and patch-based discriminators.
3. Transformer for English-to-Urdu: Discussed data tokenization, loading
a pretrained model, and the training loop.
4. ViT vs. CNN on CIFAR-10: Provided a simple CNN architecture and a
minimal Vision Transformer class for comparison.
Instead of specialized code listings, we explained each step in plain text to
clarify exactly what the code is doing and why. This approach can be adapted to
any deep learning (or general programming) project, giving readers a descriptive
line-by-line breakdown of the logic.

Acknowledgments. We thank the open-source communities for PyTorch, Trans-
formers, and other tools used in these implementations.
